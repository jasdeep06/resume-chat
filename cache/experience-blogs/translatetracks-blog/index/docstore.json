{"docstore/data": {"79de6ee3-106d-489b-968e-62eabfafaf87": {"__data__": {"id_": "79de6ee3-106d-489b-968e-62eabfafaf87", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e102709b-3f80-4a93-97ef-5fefbea14a97", "node_type": "1", "metadata": {}, "hash": "fc5ea429dea57620f6fc650cfa89e0b54f23c63509f063645771cb87592e3593", "class_name": "RelatedNodeInfo"}}, "text": "4/9/24, 7:56 PM                                                                      React App\n         TranslateTracks (AI Dubbing with Human-in-loop) - A\n        About TranslateTracks                           Technical Report\n        Given the development of highly logically capable LLMs enabling SOTA translation and human-like high fidelity AI\n        Text-to-speech services like ElevenLabs, we could see the immense impact this would create in the dubbing industry,\n        where both AI translation and AI Speech could completely change the industry landscape. Given this thesis in our mind,\n        we started working on Translatetracks.\n        TranslateTracks ran for 7 months, did $100K in revenue with a net margin of 75%. Our Human-in-loop service stood out\n        in the clutter of one-shot Dubbing products, where the final results are far from production ready with numerous\n        inaccuracies, both in terms of translations and the final dubs. We created a network of translators and proofreaders for\n        different European languages in India, thus letting us deliver the service at a fraction of the cost of our competitors.", "start_char_idx": 0, "end_char_idx": 1151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e102709b-3f80-4a93-97ef-5fefbea14a97": {"__data__": {"id_": "e102709b-3f80-4a93-97ef-5fefbea14a97", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79de6ee3-106d-489b-968e-62eabfafaf87", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "79fd1ab113dd48a3aa1a8c7a8067e7e8d02f14aa82947c24e4e268ff64ea23ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa2d269e-39ae-419f-befb-3d57028a9124", "node_type": "1", "metadata": {}, "hash": "5b4ae32b490065b1d508b91b649e914850a848c73e26b244f3a67213ff9561fa", "class_name": "RelatedNodeInfo"}}, "text": "To\n        be precise, we were 4x cheaper then our nearest competitor of comparable quality.\n        Here are a few videos we have dubbed:\n             The Catamaran Challenge\n             Tim Cook Interview (change audio track to english)\n             Jamy\u2019s Coffee Effects (change audio track to english)\n        Technical Description\n        Introduction\n        Our platform/app basically involves four different steps:\n             Video Upload       : Direct video upload/ Upload video via Youtube link. We extract the Audio from the video file.\n         This is followed by a vocal remover process that separates the Vocal track of the audio from the Background audio,\n         giving us a Vocal Track and a Background Track.\n             Transcription      : The separated Vocal Track is transcribed using our ASR Engine (Automatic Speech Recognition)\n         that uses multiple SOTA transcription APIs like AssemblyAI, Deepgram and Gladia.\n             Translation     : Once the Video is transcribed, and the segments are verified, we translate the text from source\n         language to the target language, using an LLM of our choice (Typically Chatgpt4, Claude, and LLama2).", "start_char_idx": 1152, "end_char_idx": 2339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fa2d269e-39ae-419f-befb-3d57028a9124": {"__data__": {"id_": "fa2d269e-39ae-419f-befb-3d57028a9124", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e102709b-3f80-4a93-97ef-5fefbea14a97", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44e52f936f05bcf549827d210c803e6aa294cafe1f7892461a6fd72c06e8d447", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a49b510-e611-4c14-97f7-35f191c2a2f5", "node_type": "1", "metadata": {}, "hash": "8b7367acc200c0d6f44b567384d8fdc5658570a9d2d13503d059e8cb33d7b972", "class_name": "RelatedNodeInfo"}}, "text": "This step\n         involves multiple LLM based optimizations, to assure that the translations are up to the mark.\n             Text-to-Speech       : This is the last step. For each translated segment, and based on the assigned speaker to the text,\n         an AI speech is generated using a TTS API (Elevenlabs) that is matched with the characteristics of the original voice.\n         This step also has multiple ML/AI optimization pipelines, that makes sure that final audio is expressive and engaging,\n         and at no point, feel machine generated.\n        Video Upload\n        Our app can be accessed      here. Video for dubbing can be uploaded directly or directly via a youtube link.\nlocalhost:3001                                                                                                                          1/64/9/24, 7:56 PM                                                                       React App\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VoiceiSHOWCASEHOW TO USE?", "start_char_idx": 2340, "end_char_idx": 3369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6a49b510-e611-4c14-97f7-35f191c2a2f5": {"__data__": {"id_": "6a49b510-e611-4c14-97f7-35f191c2a2f5", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa2d269e-39ae-419f-befb-3d57028a9124", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "90f6147dc2b78b7ebb35ed1fdf6818956438ca2362c0170da5bec093621098fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0998e203-c32e-4720-85e3-f5ac6c6ef3d4", "node_type": "1", "metadata": {}, "hash": "56094870facc1eed81f7f908a6fc7619eaea50752fe39f1dcd95ac0893fa2bbb", "class_name": "RelatedNodeInfo"}}, "text": "Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD       files\n                                                                       DragDrop    Browse\n                                                                             Youi\n                                                                         Your Videos\n                                RequestID         Title        Duratlon Target LanguageStatus       Add Backgrcund   Download\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VOICEISHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD\n                                                        Youtube URL\n                                                                            GET INFOI\n                                                                         Your Videos\n                                Request ID                     Duratlon Target LanguageSlalus       Add Background   Downbad\n       Our app is hosted on AWS EC2 Ubuntu 22 Server, backend is written in Python FastAPI,and frontend is written in\n       React, and for the database, we are using MongoDB for storing video/user data, audios/videos are hosted on S3. We use\n       various LLMs like ChatGPT, Claude and LLama2 in our application. All the LLM calls are logged in Langfuse for\n       observability, LLM performance monitoring, and running evaluation on output afterwards.", "start_char_idx": 3389, "end_char_idx": 4894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0998e203-c32e-4720-85e3-f5ac6c6ef3d4": {"__data__": {"id_": "0998e203-c32e-4720-85e3-f5ac6c6ef3d4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a49b510-e611-4c14-97f7-35f191c2a2f5", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "dca86935f0685b6baca85d815be0f70e19f2b47181c244a59c70a63d41252e2e", "class_name": "RelatedNodeInfo"}}, "text": "We are also using\n       PostgresQL for logging user platform usage/story, and NewRelic for instance health monitoring.\n       Once the video is uploaded, we use a Background remover that is deployed on the same server. For every MP4 file, we\n       get Vocal & Bg audio tracks in WAV format. This vocal file ensures that all the background music/noise is separated\n       from the audio track. This ensures high accuracy in the Transcription process as the audio is Noise-free.\n       Transcription\n            We then transcribe the audio file to convert it into text format.", "start_char_idx": 4895, "end_char_idx": 5472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5a96d76e-692d-47c8-b8be-25bef3031512": {"__data__": {"id_": "5a96d76e-692d-47c8-b8be-25bef3031512", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cec031e-c399-4ad9-8f17-77c16a24b62d", "node_type": "1", "metadata": {}, "hash": "a9e878bbb9559dd041d9f6749287d8f7e87282956fc55d15529f5a5a5d37d8c2", "class_name": "RelatedNodeInfo"}}, "text": "4/9/24, 7:56 PM                                                                      React App\n         TranslateTracks (AI Dubbing with Human-in-loop) - A\n        About TranslateTracks                           Technical Report\n        Given the development of highly logically capable LLMs enabling SOTA translation and human-like high fidelity AI\n        Text-to-speech services like ElevenLabs, we could see the immense impact this would create in the dubbing industry,\n        where both AI translation and AI Speech could completely change the industry landscape. Given this thesis in our mind,\n        we started working on Translatetracks.\n        TranslateTracks ran for 7 months, did $100K in revenue with a net margin of 75%. Our Human-in-loop service stood out\n        in the clutter of one-shot Dubbing products, where the final results are far from production ready with numerous\n        inaccuracies, both in terms of translations and the final dubs. We created a network of translators and proofreaders for\n        different European languages in India, thus letting us deliver the service at a fraction of the cost of our competitors. To\n        be precise, we were 4x cheaper then our nearest competitor of comparable quality.\n        Here are a few videos we have dubbed:\n             The Catamaran Challenge\n             Tim Cook Interview (change audio track to english)\n             Jamy\u2019s Coffee Effects (change audio track to english)\n        Technical Description\n        Introduction\n        Our platform/app basically involves four different steps:\n             Video Upload       : Direct video upload/ Upload video via Youtube link. We extract the Audio from the video file.\n         This is followed by a vocal remover process that separates the Vocal track of the audio from the Background audio,\n         giving us a Vocal Track and a Background Track.\n             Transcription      : The separated Vocal Track is transcribed using our ASR Engine (Automatic Speech Recognition)\n         that uses multiple SOTA transcription APIs like AssemblyAI, Deepgram and Gladia.\n             Translation     : Once the Video is transcribed, and the segments are verified, we translate the text from source\n         language to the target language, using an LLM of our choice (Typically Chatgpt4, Claude, and LLama2). This step\n         involves multiple LLM based optimizations, to assure that the translations are up to the mark.\n             Text-to-Speech       : This is the last step.", "start_char_idx": 0, "end_char_idx": 2512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3cec031e-c399-4ad9-8f17-77c16a24b62d": {"__data__": {"id_": "3cec031e-c399-4ad9-8f17-77c16a24b62d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a96d76e-692d-47c8-b8be-25bef3031512", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "5d9643bf5af1b6545c42ee82c2cb29bae82c63b1972951767b1e814f5b2fa2d6", "class_name": "RelatedNodeInfo"}}, "text": "For each translated segment, and based on the assigned speaker to the text,\n         an AI speech is generated using a TTS API (Elevenlabs) that is matched with the characteristics of the original voice.\n         This step also has multiple ML/AI optimization pipelines, that makes sure that final audio is expressive and engaging,\n         and at no point, feel machine generated.\n        Video Upload\n        Our app can be accessed      here. Video for dubbing can be uploaded directly or directly via a youtube link.\nlocalhost:3001                                                                                                                          1/64/9/24, 7:56 PM                                                                       React App\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VoiceiSHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD       files\n                                                                       DragDrop    Browse\n                                                                             Youi\n                                                                         Your Videos\n                                RequestID         Title        Duratlon Target LanguageStatus       Add Backgrcund   Download\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VOICEISHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD\n                                                        Youtube URL\n                                                                            GET INFOI\n                                                                         Your Videos\n                                Request ID                     Duratlon Target LanguageSlalus       Add Background   Downbad\n       Our app is hosted on AWS EC2 Ubuntu 22 Server, backend is written in Python FastAPI,and frontend is written in\n       React, and for the database, we are using MongoDB for storing video/user data, audios/videos are hosted on S3. We use\n       various LLMs like ChatGPT, Claude and LLama2 in our application. All the LLM calls are logged in Langfuse for\n       observability, LLM performance monitoring, and running evaluation on output afterwards. We are also using\n       PostgresQL for logging user platform usage/story, and NewRelic for instance health monitoring.\n       Once the video is uploaded, we use a Background remover that is deployed on the same server. For every MP4 file, we\n       get Vocal & Bg audio tracks in WAV format. This vocal file ensures that all the background music/noise is separated\n       from the audio track. This ensures high accuracy in the Transcription process as the audio is Noise-free.\n       Transcription\n            We then transcribe the audio file to convert it into text format.", "start_char_idx": 2513, "end_char_idx": 5472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0f103fd-f885-4ad4-9216-8ecb77e40518", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44cce72dfc54eb3ebcb4be1566f418d29728824ef4849223f273db80f6c9bdca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1ced7d6-9120-486b-9d9c-f99f9db88d62", "node_type": "1", "metadata": {}, "hash": "d6d27f4c1dfca4b2330e92ece0980df093745533250f83fd44e0c27aede12cc5", "class_name": "RelatedNodeInfo"}}, "text": "4/9/24, 7:56 PM                                                                      React App\n         TranslateTracks (AI Dubbing with Human-in-loop) - A\n        About TranslateTracks                           Technical Report\n        Given the development of highly logically capable LLMs enabling SOTA translation and human-like high fidelity AI\n        Text-to-speech services like ElevenLabs, we could see the immense impact this would create in the dubbing industry,\n        where both AI translation and AI Speech could completely change the industry landscape. Given this thesis in our mind,\n        we started working on Translatetracks.\n        TranslateTracks ran for 7 months, did $100K in revenue with a net margin of 75%. Our Human-in-loop service stood out\n        in the clutter of one-shot Dubbing products, where the final results are far from production ready with numerous\n        inaccuracies, both in terms of translations and the final dubs. We created a network of translators and proofreaders for\n        different European languages in India, thus letting us deliver the service at a fraction of the cost of our competitors. To\n        be precise, we were 4x cheaper then our nearest competitor of comparable quality.\n        Here are a few videos we have dubbed:\n             The Catamaran Challenge\n             Tim Cook Interview (change audio track to english)\n             Jamy\u2019s Coffee Effects (change audio track to english)\n        Technical Description\n        Introduction\n        Our platform/app basically involves four different steps:\n             Video Upload       : Direct video upload/ Upload video via Youtube link. We extract the Audio from the video file.\n         This is followed by a vocal remover process that separates the Vocal track of the audio from the Background audio,\n         giving us a Vocal Track and a Background Track.\n             Transcription      : The separated Vocal Track is transcribed using our ASR Engine (Automatic Speech Recognition)\n         that uses multiple SOTA transcription APIs like AssemblyAI, Deepgram and Gladia.\n             Translation     : Once the Video is transcribed, and the segments are verified, we translate the text from source\n         language to the target language, using an LLM of our choice (Typically Chatgpt4, Claude, and LLama2). This step\n         involves multiple LLM based optimizations, to assure that the translations are up to the mark.\n             Text-to-Speech       : This is the last step. For each translated segment, and based on the assigned speaker to the text,\n         an AI speech is generated using a TTS API (Elevenlabs) that is matched with the characteristics of the original voice.\n         This step also has multiple ML/AI optimization pipelines, that makes sure that final audio is expressive and engaging,\n         and at no point, feel machine generated.\n        Video Upload\n        Our app can be accessed      here. Video for dubbing can be uploaded directly or directly via a youtube link.\nlocalhost:3001                                                                                                                          1/64/9/24, 7:56 PM                                                                       React App\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VoiceiSHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD       files\n                                                                       DragDrop    Browse\n                                                                             Youi\n                                                                         Your Videos\n                                RequestID         Title        Duratlon Target LanguageStatus       Add Backgrcund   Download\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VOICEISHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD\n                                                        Youtube URL\n                                                                            GET INFOI\n                                                                         Your Videos\n                                Request ID                     Duratlon Target LanguageSlalus       Add Background   Downbad\n       Our app is hosted on AWS EC2 Ubuntu 22 Server, backend is written in Python FastAPI,and frontend is written in\n       React, and for the database, we are using MongoDB for storing video/user data, audios/videos are hosted on S3. We use\n       various LLMs like ChatGPT, Claude and LLama2 in our application. All the LLM calls are logged in Langfuse for\n       observability, LLM performance monitoring, and running evaluation on output afterwards. We are also using\n       PostgresQL for logging user platform usage/story, and NewRelic for instance health monitoring.\n       Once the video is uploaded, we use a Background remover that is deployed on the same server. For every MP4 file, we\n       get Vocal & Bg audio tracks in WAV format. This vocal file ensures that all the background music/noise is separated\n       from the audio track. This ensures high accuracy in the Transcription process as the audio is Noise-free.\n       Transcription\n            We then transcribe the audio file to convert it into text format.", "start_char_idx": 0, "end_char_idx": 5472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "215f198e-2c13-4dfc-ad5f-55a9f09c806b": {"__data__": {"id_": "215f198e-2c13-4dfc-ad5f-55a9f09c806b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3932f1d-5174-4656-8b16-977e7dabe5d6", "node_type": "1", "metadata": {}, "hash": "ceea5b287db3ebea2c12a9d8804fbf12533fe7090f01264aa40f59bbe09b04d1", "class_name": "RelatedNodeInfo"}}, "text": "Along with generating text, this process also assigns\n        a unique speaker label to each text segment, this process is called Speaker Diarization.\n            For this we use a combination of different Speech to text APIs to get the most accurate transcripts for our video.\n        We use Deepgram, AssemblyAI and Gladia for transcription. Every service has its own merits and demerits.\n            We use our ASR Engine (Automatic speech recognition), that takes input from all three services, and optimises the\n        output, so that we can minimise the Word Error rate, Reduce Hallucinations and maximise the Diarisation accuracy.\n            All the optimization pipelines in our application results in one thing, and that is Lip Synchronized / Lip matched\n        final Audio.\n            To ensure this, we split the transcript segments into smaller sub-segments, so that each segment appropriately\n        matches with the content on the screen.\n            We automatically split these segments at Audio and Video Breakpoints. These are the points where either the\n        Silence is greater than Threshold, or there is scene change.", "start_char_idx": 0, "end_char_idx": 1146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a3932f1d-5174-4656-8b16-977e7dabe5d6": {"__data__": {"id_": "a3932f1d-5174-4656-8b16-977e7dabe5d6", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "215f198e-2c13-4dfc-ad5f-55a9f09c806b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "0f7f75fc345315c3531611ac9c33d45e272d0e935db6cc97d684df189466720d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33046f41-e9af-4ff3-b1f7-79737fc163e8", "node_type": "1", "metadata": {}, "hash": "32651eecd783710d5b4c669c965664c21384a0749ef0180cba7d77d03a1239bc", "class_name": "RelatedNodeInfo"}}, "text": "localhost:3001                                                                                                                        2/64/9/24, 7:56 PM                                                          React App    emotion\n                                                    [CAT\n                                                             Video Segment Duralion: 5.945\n                                     Transcription\n                                    Benvenuli in Italia,coslicra amallianauno dci piu bei tratliabbiamo\n                                                          cosia cncqueslo pacse               X 1\n                                       WelcomeItaly; on Ine Amalli Coast, one of the most beautiful stretches of coaslline we have in Ihis country:\n                                     Translalion               TRANSLATESAVE                 Audio Bp\n                                                             Thave assigned the speaker mapping:\n           Finally, we see the segments in the timeline of the editor, along with the Video and audio. Now we have a human in\n        the loop to correct the Transcripts. The editor provides multiple functionalities to modify various aspects of the\n        transcripts.                                         Video to Dub\n                                                          Direct Upload / Youtube link\n                            Extract Audio via FFmpeg         Video Source                    Technologies used -\n                                                                                   ffmpeg - For extracting audio from video\n                               Vocal Remover                                    and to detect Audio and Video breakpoints.", "start_char_idx": 1147, "end_char_idx": 2895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "33046f41-e9af-4ff3-b1f7-79737fc163e8": {"__data__": {"id_": "33046f41-e9af-4ff3-b1f7-79737fc163e8", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3932f1d-5174-4656-8b16-977e7dabe5d6", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "72d97b448948bbf0aebbd6156c3e9351957bb43cdd70e84c29283399903f4225", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cc6d2ac-d1f3-4bb1-9029-8b9f5ee0560f", "node_type": "1", "metadata": {}, "hash": "892b5077333467e84a364e9b9ac96e444ec9c5d633d23792af2cf272287c614b", "class_name": "RelatedNodeInfo"}}, "text": "Vocal Track               Background Track               ASR services - Assembly AI, Deepgram,\n                                                                                Gladia.\n                                                                         Video FileVocalRemover(https://vocalremover.org/)\n                                                        Video Breakpoints using FFmpeg\n        AssemblyAIDeepgram Gladia  Audio Breakpoints using FFmpeg               - A service to seperate vocals from\n                     ASR Engine                    Audio File                   background.\n                   Optimized Transcript                                            Python Libraries - moviepy,pydub\n       Translation Processed Transcript      Dubbing Editor\n           Once transcription is verified, the next step is Translation.\n           The most fundamental requirement here is that, the #segments in Transcript and #Segments in Translation should\n        be the same.", "start_char_idx": 2922, "end_char_idx": 3914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4cc6d2ac-d1f3-4bb1-9029-8b9f5ee0560f": {"__data__": {"id_": "4cc6d2ac-d1f3-4bb1-9029-8b9f5ee0560f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33046f41-e9af-4ff3-b1f7-79737fc163e8", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "4590529e6de047fc5ec7ba5681510c1f1d6e075db4c935f5dcdefb7948144ee4", "class_name": "RelatedNodeInfo"}}, "text": "localhost:3001                                                                                                                     3/64/9/24, 7:56 PM                                                                React App\n                               \"1\" \"Bienvenidostodos                         1: \"Welcome everyone_\n                               \"2\" \"Hoy ,    vluodeddeode hoy ,   hablar\n                                        en                 vamos                \"Today ,        video,\n                               \"3\" \"E pues bueno,     mencionados                     in today        Were going to\n                                               m |uopels       delmundo                        most mentioned\n                                                    de la relojer |uadeda       \"Well,oneof the             inthe\n                               \"4\" \"Sobretodo delmundo                                                 of watchmaking\n                                                     la relojerluggeda       4: \"Especially from the world\n                                   \"de entrada,                    de luj\n                                             un poco                         5: \"starting     bit into the world oflu;\n                                                        Ilevar relojes                  out,\n                                   \"Es   calibre  suelen           desde\n                                      un       que                                    caliberthat is foundin Watches\n                              euros                                          6: \"It\\' s         large group.", "start_char_idx": 3915, "end_char_idx": 5562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1252e41b-96f5-416b-8d30-8db5fa7b0200": {"__data__": {"id_": "1252e41b-96f5-416b-8d30-8db5fa7b0200", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddcbc84b-3c4c-4bc5-922d-46e788244b87", "node_type": "1", "metadata": {}, "hash": "a581d32aa881a620f51340cf4175e04c73bfc7dc1189f977667ee9ec5d3334b9", "class_name": "RelatedNodeInfo"}}, "text": "Along with generating text, this process also assigns\n        a unique speaker label to each text segment, this process is called Speaker Diarization.\n            For this we use a combination of different Speech to text APIs to get the most accurate transcripts for our video.\n        We use Deepgram, AssemblyAI and Gladia for transcription. Every service has its own merits and demerits.\n            We use our ASR Engine (Automatic speech recognition), that takes input from all three services, and optimises the\n        output, so that we can minimise the Word Error rate, Reduce Hallucinations and maximise the Diarisation accuracy.\n            All the optimization pipelines in our application results in one thing, and that is Lip Synchronized / Lip matched\n        final Audio.\n            To ensure this, we split the transcript segments into smaller sub-segments, so that each segment appropriately\n        matches with the content on the screen.\n            We automatically split these segments at Audio and Video Breakpoints. These are the points where either the\n        Silence is greater than Threshold, or there is scene change.\nlocalhost:3001                                                                                                                        2/64/9/24, 7:56 PM                                                          React App    emotion\n                                                    [CAT\n                                                             Video Segment Duralion: 5.945\n                                     Transcription\n                                    Benvenuli in Italia,coslicra amallianauno dci piu bei tratliabbiamo\n                                                          cosia cncqueslo pacse               X 1\n                                       WelcomeItaly; on Ine Amalli Coast, one of the most beautiful stretches of coaslline we have in Ihis country:\n                                     Translalion               TRANSLATESAVE                 Audio Bp\n                                                             Thave assigned the speaker mapping:\n           Finally, we see the segments in the timeline of the editor, along with the Video and audio. Now we have a human in\n        the loop to correct the Transcripts. The editor provides multiple functionalities to modify various aspects of the\n        transcripts.                                         Video to Dub\n                                                          Direct Upload / Youtube link\n                            Extract Audio via FFmpeg         Video Source                    Technologies used -\n                                                                                   ffmpeg - For extracting audio from video\n                               Vocal Remover                                    and to detect Audio and Video breakpoints.\n                          Vocal Track               Background Track               ASR services - Assembly AI, Deepgram,\n                                                                                Gladia.", "start_char_idx": 0, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ddcbc84b-3c4c-4bc5-922d-46e788244b87": {"__data__": {"id_": "ddcbc84b-3c4c-4bc5-922d-46e788244b87", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1252e41b-96f5-416b-8d30-8db5fa7b0200", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "affadbf1ad78eb820b199dda631fd447a8bd37c3e52a6b7031ea3dc39dad999e", "class_name": "RelatedNodeInfo"}}, "text": "Video FileVocalRemover(https://vocalremover.org/)\n                                                        Video Breakpoints using FFmpeg\n        AssemblyAIDeepgram Gladia  Audio Breakpoints using FFmpeg               - A service to seperate vocals from\n                     ASR Engine                    Audio File                   background.\n                   Optimized Transcript                                            Python Libraries - moviepy,pydub\n       Translation Processed Transcript      Dubbing Editor\n           Once transcription is verified, the next step is Translation.\n           The most fundamental requirement here is that, the #segments in Transcript and #Segments in Translation should\n        be the same.\nlocalhost:3001                                                                                                                     3/64/9/24, 7:56 PM                                                                React App\n                               \"1\" \"Bienvenidostodos                         1: \"Welcome everyone_\n                               \"2\" \"Hoy ,    vluodeddeode hoy ,   hablar\n                                        en                 vamos                \"Today ,        video,\n                               \"3\" \"E pues bueno,     mencionados                     in today        Were going to\n                                               m |uopels       delmundo                        most mentioned\n                                                    de la relojer |uadeda       \"Well,oneof the             inthe\n                               \"4\" \"Sobretodo delmundo                                                 of watchmaking\n                                                     la relojerluggeda       4: \"Especially from the world\n                                   \"de entrada,                    de luj\n                                             un poco                         5: \"starting     bit into the world oflu;\n                                                        Ilevar relojes                  out,\n                                   \"Es   calibre  suelen           desde\n                                      un       que                                    caliberthat is foundin Watches\n                              euros                                          6: \"It\\' s         large group.", "start_char_idx": 3178, "end_char_idx": 5562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0f103fd-f885-4ad4-9216-8ecb77e40518", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44cce72dfc54eb3ebcb4be1566f418d29728824ef4849223f273db80f6c9bdca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "907a8529-9912-40df-a780-9759369abf61", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5dec52b-4b2b-46fa-9dcd-fc9fa8e67296", "node_type": "1", "metadata": {}, "hash": "68b8a7ef225bdd8f3f9ef14954510532f6d040ef41b77a64e13dc60e5caeceff", "class_name": "RelatedNodeInfo"}}, "text": "Along with generating text, this process also assigns\n        a unique speaker label to each text segment, this process is called Speaker Diarization.\n            For this we use a combination of different Speech to text APIs to get the most accurate transcripts for our video.\n        We use Deepgram, AssemblyAI and Gladia for transcription. Every service has its own merits and demerits.\n            We use our ASR Engine (Automatic speech recognition), that takes input from all three services, and optimises the\n        output, so that we can minimise the Word Error rate, Reduce Hallucinations and maximise the Diarisation accuracy.\n            All the optimization pipelines in our application results in one thing, and that is Lip Synchronized / Lip matched\n        final Audio.\n            To ensure this, we split the transcript segments into smaller sub-segments, so that each segment appropriately\n        matches with the content on the screen.\n            We automatically split these segments at Audio and Video Breakpoints. These are the points where either the\n        Silence is greater than Threshold, or there is scene change.\nlocalhost:3001                                                                                                                        2/64/9/24, 7:56 PM                                                          React App    emotion\n                                                    [CAT\n                                                             Video Segment Duralion: 5.945\n                                     Transcription\n                                    Benvenuli in Italia,coslicra amallianauno dci piu bei tratliabbiamo\n                                                          cosia cncqueslo pacse               X 1\n                                       WelcomeItaly; on Ine Amalli Coast, one of the most beautiful stretches of coaslline we have in Ihis country:\n                                     Translalion               TRANSLATESAVE                 Audio Bp\n                                                             Thave assigned the speaker mapping:\n           Finally, we see the segments in the timeline of the editor, along with the Video and audio. Now we have a human in\n        the loop to correct the Transcripts. The editor provides multiple functionalities to modify various aspects of the\n        transcripts.                                         Video to Dub\n                                                          Direct Upload / Youtube link\n                            Extract Audio via FFmpeg         Video Source                    Technologies used -\n                                                                                   ffmpeg - For extracting audio from video\n                               Vocal Remover                                    and to detect Audio and Video breakpoints.\n                          Vocal Track               Background Track               ASR services - Assembly AI, Deepgram,\n                                                                                Gladia.\n                                                                         Video FileVocalRemover(https://vocalremover.org/)\n                                                        Video Breakpoints using FFmpeg\n        AssemblyAIDeepgram Gladia  Audio Breakpoints using FFmpeg               - A service to seperate vocals from\n                     ASR Engine                    Audio File                   background.\n                   Optimized Transcript                                            Python Libraries - moviepy,pydub\n       Translation Processed Transcript      Dubbing Editor\n           Once transcription is verified, the next step is Translation.\n           The most fundamental requirement here is that, the #segments in Transcript and #Segments in Translation should\n        be the same.\nlocalhost:3001                                                                                                                     3/64/9/24, 7:56 PM                                                                React App\n                               \"1\" \"Bienvenidostodos                         1: \"Welcome everyone_\n                               \"2\" \"Hoy ,    vluodeddeode hoy ,   hablar\n                                        en                 vamos                \"Today ,        video,\n                               \"3\" \"E pues bueno,     mencionados                     in today        Were going to\n                                               m |uopels       delmundo                        most mentioned\n                                                    de la relojer |uadeda       \"Well,oneof the             inthe\n                               \"4\" \"Sobretodo delmundo                                                 of watchmaking\n                                                     la relojerluggeda       4: \"Especially from the world\n                                   \"de entrada,                    de luj\n                                             un poco                         5: \"starting     bit into the world oflu;\n                                                        Ilevar relojes                  out,\n                                   \"Es   calibre  suelen           desde\n                                      un       que                                    caliberthat is foundin Watches\n                              euros                                          6: \"It\\' s         large group.", "start_char_idx": 5473, "end_char_idx": 11035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dcaecc73-0d2c-413b-9ff2-933f8941c60b": {"__data__": {"id_": "dcaecc73-0d2c-413b-9ff2-933f8941c60b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1c3ca95-594c-4a15-9c16-7e1e614f645d", "node_type": "1", "metadata": {}, "hash": "10b78f3a8363c9e13aba6f1bc11432e32783528691622257cc46b5764d90ad77", "class_name": "RelatedNodeInfo"}}, "text": "que  usadoporun grangrupo _              7: 'and it's used by\n                               \"8\"             habrlunoe9isvistolo              And well,  you        in the title\n                                     bueno,como               en   que                   as    ve seen           ana\n                              Powermatic 80                                     \"The ideaof this video is\n                               \"9\"    ideade estevluggeddeo                      \"to give    bit of general\n                                    La                   es_                 10:        you               contextabr\n                                                  contexto general\n                               \"10\"  daros  poco               sobre\n                                         un                                  11: 'what its variants  what itsmost\n                                                 sus variantes,culuedelles                       are             oui\n                               \"11   culuoellesson                                       comes from, what changes it ha:\n                                    de dludaf3ndeviene,", "start_char_idx": 0, "end_char_idx": 1168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d1c3ca95-594c-4a15-9c16-7e1e614f645d": {"__data__": {"id_": "d1c3ca95-594c-4a15-9c16-7e1e614f645d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcaecc73-0d2c-413b-9ff2-933f8941c60b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "f91ce012666760362d9a6ccb54efcc94996b1284fbb6f138d840f82bf2d2b243", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8b1300e-16ec-4cfb-b28a-a1e49795ea20", "node_type": "1", "metadata": {}, "hash": "a213eeaaf42cf8a67e76547162581d7cd64362a7162ab91864ad97f4fb7521c9", "class_name": "RelatedNodeInfo"}}, "text": "quluade9             12: 'where it\n                               \"12\"                         cambiostiene     different typesare\n                                 distintos\n                              los        tipos                                   \"I\\'1l try\n                               \"13   Intentarlugge9         denso  vlugge    13:         to make the video nottoo dense\n                                                que   seamuy                         more enjoyable,\n                               \"14       seam|uadels Ilevadero,      part    14: and              but from this video\n                                      que                   peroque          15: \"recognize _\n                               \"15\"  reconocer\n                               \"16\" \"diferentes                              16:\"different\n                                                                                        or With different\n                                    'relojes   diferentes                    17: \"watches\n                               \"17           con        movimientos                                     movements\n           We convert the input transcript to a JSON, split them into groups of maximum 50 segments, and have overlaps in\n        segment groups to preserve translation context.", "start_char_idx": 1169, "end_char_idx": 2493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c8b1300e-16ec-4cfb-b28a-a1e49795ea20": {"__data__": {"id_": "c8b1300e-16ec-4cfb-b28a-a1e49795ea20", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1c3ca95-594c-4a15-9c16-7e1e614f645d", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c723334d49de28262266d58b21bbd65e57ab7304b8f4485b680da98d1bb2540a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39da8c3b-7e85-4d4a-aa8e-d735b0c94761", "node_type": "1", "metadata": {}, "hash": "40bdd99a52ee07f95827af2b579d422ac0b98b40d64cfc573bf75cbad8f228dc", "class_name": "RelatedNodeInfo"}}, "text": "For groups with more than 50 segments, the LLMs suffer from a\n        needle in a stack issue, with poor performance, unequal input outputs, and sometimes exceed input tokens limit.\n                                           Verified Transcript\n                                             Preprocessing\n                                  Group     Split into Groups\n                         LLM :Contextual Translation::                                                            Technologies used -\n                                                                                          LLM - OpenAI gpt-4 for contextual translation\n           LLM Validator ::JSON and Balanced I/0:    Yes           Groups 2-n             Orchestrator -\n                                Fail                                                   Langchain(LLMChain,FewShotPromptTemplate,Retr\n                                                                                          Embeddings - gpt-3.5 embeddings(small 1536 dim\n                                        #Retry 32                                      mapping engine to map input and output segments in\n                       Pass                No                                          RetryOutputParser fails.\n                                  Semantic Mapping Engine   Same Process for Groups 2-n   Python Libraries - Langchain\n                                  Grouped Translation Block\n                                Postprocess :  Split Translations:\n           Every group is translated in one LLM prompt, so that Translation context is preserved.", "start_char_idx": 2494, "end_char_idx": 4098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "39da8c3b-7e85-4d4a-aa8e-d735b0c94761": {"__data__": {"id_": "39da8c3b-7e85-4d4a-aa8e-d735b0c94761", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8b1300e-16ec-4cfb-b28a-a1e49795ea20", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "2111d9c7471ccf6983fb0245696eb8ab5fe89a65d1f27fdfaa3deb0ac97fe2c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bfd716f-98c1-4cfd-b696-18f5dc9b0a2a", "node_type": "1", "metadata": {}, "hash": "1ecc0b33f5fb21a4dd6621e5ba9902e225ed8327cd723441164c7c6c1596df6a", "class_name": "RelatedNodeInfo"}}, "text": "We orchestrate this using\n        Langchain, with a Few Shots example, along with Output Parser to force the output to the defined formats.\n           We have a separate Semantic Mapping Engine, to map the input transcript and output translation in case there is a\n        mismatch between the number of segments in reference and translated group.\n           Once each group is translated, we recombine the groups to get the translated segments. Every segment is assigned\n        a speaker, that is mapped to an AI voice.\nlocalhost:3001                                                                                                                             4/64/9/24, 7:56 PM                                                                     React App\n            Now the main challenge with translation is that the same text, when translated across languages, has different\n         numbers of characters, and therefore different time to say the same thing. For example:\n                          0:07\n                          0:07 / 0:07\n                               / 0:07                                                         0:09\n                                                                                              0:09 / 0:09\n                                      English                                                      / 0:09  Hindi\n            To handle this, we have a separate LLM engine that utilises our voice models.", "start_char_idx": 4099, "end_char_idx": 5553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5bfd716f-98c1-4cfd-b696-18f5dc9b0a2a": {"__data__": {"id_": "5bfd716f-98c1-4cfd-b696-18f5dc9b0a2a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39da8c3b-7e85-4d4a-aa8e-d735b0c94761", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1e9ab8847dd98e05862f5e5f8babb5d9e02ad955295762dc76214ec2949d4fcf", "class_name": "RelatedNodeInfo"}}, "text": "For every voice, we have developed\n         ML models across languages to predict with ~80% accuracy, the expected duration of the generated audio from a\n         particular text.                                                                          Technologies used -\n                        Postprocess :: Split Translations: =                      scikit-learn, pytorch - LinerRegressionModel and\n                                                Translation #1                SVM for predicting duration of audio.", "start_char_idx": 5554, "end_char_idx": 6073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "226c20cf-6cca-4951-a2b5-f718fb294e32": {"__data__": {"id_": "226c20cf-6cca-4951-a2b5-f718fb294e32", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "043032aa-2f21-4e1a-b09a-449a4a967c93", "node_type": "1", "metadata": {}, "hash": "cc5ee3d9303cebeea29cf2d172258cc42b2e8a7cba834a002f6ca9c32932a728", "class_name": "RelatedNodeInfo"}}, "text": "que  usadoporun grangrupo _              7: 'and it's used by\n                               \"8\"             habrlunoe9isvistolo              And well,  you        in the title\n                                     bueno,como               en   que                   as    ve seen           ana\n                              Powermatic 80                                     \"The ideaof this video is\n                               \"9\"    ideade estevluggeddeo                      \"to give    bit of general\n                                    La                   es_                 10:        you               contextabr\n                                                  contexto general\n                               \"10\"  daros  poco               sobre\n                                         un                                  11: 'what its variants  what itsmost\n                                                 sus variantes,culuedelles                       are             oui\n                               \"11   culuoellesson                                       comes from, what changes it ha:\n                                    de dludaf3ndeviene, quluade9             12: 'where it\n                               \"12\"                         cambiostiene     different typesare\n                                 distintos\n                              los        tipos                                   \"I\\'1l try\n                               \"13   Intentarlugge9         denso  vlugge    13:         to make the video nottoo dense\n                                                que   seamuy                         more enjoyable,\n                               \"14       seam|uadels Ilevadero,      part    14: and              but from this video\n                                      que                   peroque          15: \"recognize _\n                               \"15\"  reconocer\n                               \"16\" \"diferentes                              16:\"different\n                                                                                        or With different\n                                    'relojes   diferentes                    17: \"watches\n                               \"17           con        movimientos                                     movements\n           We convert the input transcript to a JSON, split them into groups of maximum 50 segments, and have overlaps in\n        segment groups to preserve translation context. For groups with more than 50 segments, the LLMs suffer from a\n        needle in a stack issue, with poor performance, unequal input outputs, and sometimes exceed input tokens limit.", "start_char_idx": 0, "end_char_idx": 2675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "043032aa-2f21-4e1a-b09a-449a4a967c93": {"__data__": {"id_": "043032aa-2f21-4e1a-b09a-449a4a967c93", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "226c20cf-6cca-4951-a2b5-f718fb294e32", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "6f5e95b841c18a850aef4aa12ee0be93d466d74aa60ed5a854f7d66dc7ab4465", "class_name": "RelatedNodeInfo"}}, "text": "Verified Transcript\n                                             Preprocessing\n                                  Group     Split into Groups\n                         LLM :Contextual Translation::                                                            Technologies used -\n                                                                                          LLM - OpenAI gpt-4 for contextual translation\n           LLM Validator ::JSON and Balanced I/0:    Yes           Groups 2-n             Orchestrator -\n                                Fail                                                   Langchain(LLMChain,FewShotPromptTemplate,Retr\n                                                                                          Embeddings - gpt-3.5 embeddings(small 1536 dim\n                                        #Retry 32                                      mapping engine to map input and output segments in\n                       Pass                No                                          RetryOutputParser fails.\n                                  Semantic Mapping Engine   Same Process for Groups 2-n   Python Libraries - Langchain\n                                  Grouped Translation Block\n                                Postprocess :  Split Translations:\n           Every group is translated in one LLM prompt, so that Translation context is preserved. We orchestrate this using\n        Langchain, with a Few Shots example, along with Output Parser to force the output to the defined formats.\n           We have a separate Semantic Mapping Engine, to map the input transcript and output translation in case there is a\n        mismatch between the number of segments in reference and translated group.\n           Once each group is translated, we recombine the groups to get the translated segments. Every segment is assigned\n        a speaker, that is mapped to an AI voice.\nlocalhost:3001                                                                                                                             4/64/9/24, 7:56 PM                                                                     React App\n            Now the main challenge with translation is that the same text, when translated across languages, has different\n         numbers of characters, and therefore different time to say the same thing. For example:\n                          0:07\n                          0:07 / 0:07\n                               / 0:07                                                         0:09\n                                                                                              0:09 / 0:09\n                                      English                                                      / 0:09  Hindi\n            To handle this, we have a separate LLM engine that utilises our voice models. For every voice, we have developed\n         ML models across languages to predict with ~80% accuracy, the expected duration of the generated audio from a\n         particular text.                                                                          Technologies used -\n                        Postprocess :: Split Translations: =                      scikit-learn, pytorch - LinerRegressionModel and\n                                                Translation #1                SVM for predicting duration of audio.", "start_char_idx": 2719, "end_char_idx": 6073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0f103fd-f885-4ad4-9216-8ecb77e40518", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44cce72dfc54eb3ebcb4be1566f418d29728824ef4849223f273db80f6c9bdca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1ced7d6-9120-486b-9d9c-f99f9db88d62", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ab72283-c361-4162-9757-efe561dad152", "node_type": "1", "metadata": {}, "hash": "6bb53f7657b4e4ad1ae5b6456ca6eb7dff2979fe423d19fd12723a32e192eb73", "class_name": "RelatedNodeInfo"}}, "text": "que  usadoporun grangrupo _              7: 'and it's used by\n                               \"8\"             habrlunoe9isvistolo              And well,  you        in the title\n                                     bueno,como               en   que                   as    ve seen           ana\n                              Powermatic 80                                     \"The ideaof this video is\n                               \"9\"    ideade estevluggeddeo                      \"to give    bit of general\n                                    La                   es_                 10:        you               contextabr\n                                                  contexto general\n                               \"10\"  daros  poco               sobre\n                                         un                                  11: 'what its variants  what itsmost\n                                                 sus variantes,culuedelles                       are             oui\n                               \"11   culuoellesson                                       comes from, what changes it ha:\n                                    de dludaf3ndeviene, quluade9             12: 'where it\n                               \"12\"                         cambiostiene     different typesare\n                                 distintos\n                              los        tipos                                   \"I\\'1l try\n                               \"13   Intentarlugge9         denso  vlugge    13:         to make the video nottoo dense\n                                                que   seamuy                         more enjoyable,\n                               \"14       seam|uadels Ilevadero,      part    14: and              but from this video\n                                      que                   peroque          15: \"recognize _\n                               \"15\"  reconocer\n                               \"16\" \"diferentes                              16:\"different\n                                                                                        or With different\n                                    'relojes   diferentes                    17: \"watches\n                               \"17           con        movimientos                                     movements\n           We convert the input transcript to a JSON, split them into groups of maximum 50 segments, and have overlaps in\n        segment groups to preserve translation context. For groups with more than 50 segments, the LLMs suffer from a\n        needle in a stack issue, with poor performance, unequal input outputs, and sometimes exceed input tokens limit.\n                                           Verified Transcript\n                                             Preprocessing\n                                  Group     Split into Groups\n                         LLM :Contextual Translation::                                                            Technologies used -\n                                                                                          LLM - OpenAI gpt-4 for contextual translation\n           LLM Validator ::JSON and Balanced I/0:    Yes           Groups 2-n             Orchestrator -\n                                Fail                                                   Langchain(LLMChain,FewShotPromptTemplate,Retr\n                                                                                          Embeddings - gpt-3.5 embeddings(small 1536 dim\n                                        #Retry 32                                      mapping engine to map input and output segments in\n                       Pass                No                                          RetryOutputParser fails.\n                                  Semantic Mapping Engine   Same Process for Groups 2-n   Python Libraries - Langchain\n                                  Grouped Translation Block\n                                Postprocess :  Split Translations:\n           Every group is translated in one LLM prompt, so that Translation context is preserved. We orchestrate this using\n        Langchain, with a Few Shots example, along with Output Parser to force the output to the defined formats.\n           We have a separate Semantic Mapping Engine, to map the input transcript and output translation in case there is a\n        mismatch between the number of segments in reference and translated group.\n           Once each group is translated, we recombine the groups to get the translated segments. Every segment is assigned\n        a speaker, that is mapped to an AI voice.\nlocalhost:3001                                                                                                                             4/64/9/24, 7:56 PM                                                                     React App\n            Now the main challenge with translation is that the same text, when translated across languages, has different\n         numbers of characters, and therefore different time to say the same thing. For example:\n                          0:07\n                          0:07 / 0:07\n                               / 0:07                                                         0:09\n                                                                                              0:09 / 0:09\n                                      English                                                      / 0:09  Hindi\n            To handle this, we have a separate LLM engine that utilises our voice models. For every voice, we have developed\n         ML models across languages to predict with ~80% accuracy, the expected duration of the generated audio from a\n         particular text.                                                                          Technologies used -\n                        Postprocess :: Split Translations: =                      scikit-learn, pytorch - LinerRegressionModel and\n                                                Translation #1                SVM for predicting duration of audio.", "start_char_idx": 11072, "end_char_idx": 17145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4f46f98e-9462-429e-97c2-acf33aae6fff": {"__data__": {"id_": "4f46f98e-9462-429e-97c2-acf33aae6fff", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e06aa50-e800-438f-9ca6-aeb533513341", "node_type": "1", "metadata": {}, "hash": "7d4cc9c83f8185ffa666297dac3e274a10bc5ce7cf529fe403eadccac5dd3cd5", "class_name": "RelatedNodeInfo"}}, "text": "(voice models)\n                                             Translation Segment#1                Orchestrator - Langchain- A custom\n                Translation #2-n                                              agent(AgentExecutor) - CATTM(Context Aware Text\n                                                                              Time Matcher) with 3 custom tools(@tools)-\n                            Target timeTranslation ContextTranslationVoice Model\n                                                                 Predicted time      Text Adjustment Tool - A tool built with\n           Same process for Segment #2-n  LLM Agent :: Text Adjustment::         LLMChain which takes input as contexual\n                               Low At          High At                           translation and gives out smaller or larger text as\n            Processed Translation Segment   Text Adjustment                      required.\n                                                             Updated Predicted time  GPT-4 evaluator Tool - A tool built with\n                                                  Validation Fail                load_evaluator which uses gpt-4 to critique the\n                                            Self Critiqe LLM                     quality of adjusted text given by Text Adjustment\n                                             Validation Pass                     tool.\n                                                    Voice Model                      Voice Model Tool - A tool that takes in text and\n                                                                                 speaker as input and spits out the time required to\n                                                                                 speak that text by that speaker.", "start_char_idx": 0, "end_char_idx": 1784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4e06aa50-e800-438f-9ca6-aeb533513341": {"__data__": {"id_": "4e06aa50-e800-438f-9ca6-aeb533513341", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f46f98e-9462-429e-97c2-acf33aae6fff", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "e3d1361e1400459543958b5964eade7fb9be874015e7d9cb37265740cf98fa0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0feae351-fc2c-4bf9-9066-0bfe55f3798f", "node_type": "1", "metadata": {}, "hash": "33979c950e3d14fb9e702e53b0e7be7beaed5687d0643d0ecb21699ae1217dbb", "class_name": "RelatedNodeInfo"}}, "text": "LLM - LLama2-7b for text adjustment(finetuned\n                                                                              using LORA), GPT-4 for evaluating outputs of llama2-\n                                                                              7b. HuggingFace - PEFT for finetuning llam2-7b using\n                                                                              LORA on custom dataset of shortening and lengthening\n                                                                              text.\n            Utilising those models, we run this process for every translated segment, and modify the translation based on the\n         acceptable limits of \u0394t, for every voice.\n            We have a LLM agent to adjust the translation, and a Self critique LLM to judge the modified translation on\n         parameters such as Grammar, Context loss, Colloquialism etc. Once the translations are modified, these are reviewed\n         by a Human in the loop for final validation.\n       Audio Generation\n            Once the translations are verified, we generate audio for each translated segment. To ensure that the final dubbing\n         is accurate, engaging and preserves the emotions from the original track, we have another LLM engine, that adjusts\n         translation based on continuous feedback, and regenerates audio multiple times to achieve this target.", "start_char_idx": 1867, "end_char_idx": 3253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0feae351-fc2c-4bf9-9066-0bfe55f3798f": {"__data__": {"id_": "0feae351-fc2c-4bf9-9066-0bfe55f3798f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e06aa50-e800-438f-9ca6-aeb533513341", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c30b03e96e1a992054d79d96345a534079cdbfd5a91be732dcc6ff9e3c64fd71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d8df8ac-c04c-4d78-b46c-b2c4ec662b83", "node_type": "1", "metadata": {}, "hash": "7baf1dbf21907eb22cd19829cbab9ed48cfb7fd2f699c16b3aceaccaa15a8169", "class_name": "RelatedNodeInfo"}}, "text": "localhost:3001                                                                                                                         5/64/9/24, 7:56 PM               Verified Translation segments               React App\n           Translation segment #n\n                                           Reference segment\n             Audio generation                source lang::\n                    High 4\n          actual audio time       target time\n                 LLM                                                                   Technologies used -\n              Text Adjustment:\n                 Low $                        Audio features            ElevenLabs - For Text to Speech and voice cloning.\n             Modified Translation                                       Orchestrator - Langchain - CATTM agent\n                with Audio     Translation #n-1, #n-2\n            Audio Features      Audiofeatures                           Wav2vec and SpeechBrain - For Audio similarity and\n                                                                     continuity engine.", "start_char_idx": 3254, "end_char_idx": 4339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2d8df8ac-c04c-4d78-b46c-b2c4ec662b83": {"__data__": {"id_": "2d8df8ac-c04c-4d78-b46c-b2c4ec662b83", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0feae351-fc2c-4bf9-9066-0bfe55f3798f", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "226b98d862e403ab81e12ae1e53ff8661c2bb3b5b7d36ee49325ffd51b6f16ac", "class_name": "RelatedNodeInfo"}}, "text": "LLM : Similarity                     Python Libraries - Langchain, SpeechBrain, pydub,\n                                   and Continuity  Translation segment N| {n}moviepy\n                                    Engine::\n                           Failed    Passed\n                 Audio regenerationFinal Audio for segment #nSame process as Translated #n\n                                              Translated Audio Track\n           In this process, we have a Similarity and continuity engine that takes audio features from previous generated\n        segments, as well as features from the reference audio, and regenerates audio multiple times to ensure that the output\n        audio has similar emotions to the source segment, and when played with previous segments, it should sound\n        continuous.\n           Once this process is done for every segment, the generated audios are verified by our Human experts, and they\n        ensure the final Dubbing is perfect.\nlocalhost:3001                                                                                                                  6/6", "start_char_idx": 4375, "end_char_idx": 5475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "247cdf5e-0afb-4c1c-b7d2-a408bedea1f5": {"__data__": {"id_": "247cdf5e-0afb-4c1c-b7d2-a408bedea1f5", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84562e82-e49c-45f5-8c91-260e374807d7", "node_type": "1", "metadata": {}, "hash": "f097a7688b97783e9ef6912efcee118fab47eb18e95d1416c82225dae3c268ca", "class_name": "RelatedNodeInfo"}}, "text": "(voice models)\n                                             Translation Segment#1                Orchestrator - Langchain- A custom\n                Translation #2-n                                              agent(AgentExecutor) - CATTM(Context Aware Text\n                                                                              Time Matcher) with 3 custom tools(@tools)-\n                            Target timeTranslation ContextTranslationVoice Model\n                                                                 Predicted time      Text Adjustment Tool - A tool built with\n           Same process for Segment #2-n  LLM Agent :: Text Adjustment::         LLMChain which takes input as contexual\n                               Low At          High At                           translation and gives out smaller or larger text as\n            Processed Translation Segment   Text Adjustment                      required.\n                                                             Updated Predicted time  GPT-4 evaluator Tool - A tool built with\n                                                  Validation Fail                load_evaluator which uses gpt-4 to critique the\n                                            Self Critiqe LLM                     quality of adjusted text given by Text Adjustment\n                                             Validation Pass                     tool.\n                                                    Voice Model                      Voice Model Tool - A tool that takes in text and\n                                                                                 speaker as input and spits out the time required to\n                                                                                 speak that text by that speaker.\n                                                                                  LLM - LLama2-7b for text adjustment(finetuned\n                                                                              using LORA), GPT-4 for evaluating outputs of llama2-\n                                                                              7b. HuggingFace - PEFT for finetuning llam2-7b using\n                                                                              LORA on custom dataset of shortening and lengthening\n                                                                              text.\n            Utilising those models, we run this process for every translated segment, and modify the translation based on the\n         acceptable limits of \u0394t, for every voice.\n            We have a LLM agent to adjust the translation, and a Self critique LLM to judge the modified translation on\n         parameters such as Grammar, Context loss, Colloquialism etc. Once the translations are modified, these are reviewed\n         by a Human in the loop for final validation.\n       Audio Generation\n            Once the translations are verified, we generate audio for each translated segment. To ensure that the final dubbing\n         is accurate, engaging and preserves the emotions from the original track, we have another LLM engine, that adjusts\n         translation based on continuous feedback, and regenerates audio multiple times to achieve this target.", "start_char_idx": 0, "end_char_idx": 3253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "84562e82-e49c-45f5-8c91-260e374807d7": {"__data__": {"id_": "84562e82-e49c-45f5-8c91-260e374807d7", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "247cdf5e-0afb-4c1c-b7d2-a408bedea1f5", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "8c1fb58d8021ca969ef6ca644b6865211a5ec3da14d1ee784bc221eb862c2954", "class_name": "RelatedNodeInfo"}}, "text": "localhost:3001                                                                                                                         5/64/9/24, 7:56 PM               Verified Translation segments               React App\n           Translation segment #n\n                                           Reference segment\n             Audio generation                source lang::\n                    High 4\n          actual audio time       target time\n                 LLM                                                                   Technologies used -\n              Text Adjustment:\n                 Low $                        Audio features            ElevenLabs - For Text to Speech and voice cloning.\n             Modified Translation                                       Orchestrator - Langchain - CATTM agent\n                with Audio     Translation #n-1, #n-2\n            Audio Features      Audiofeatures                           Wav2vec and SpeechBrain - For Audio similarity and\n                                                                     continuity engine.\n                                   LLM : Similarity                     Python Libraries - Langchain, SpeechBrain, pydub,\n                                   and Continuity  Translation segment N| {n}moviepy\n                                    Engine::\n                           Failed    Passed\n                 Audio regenerationFinal Audio for segment #nSame process as Translated #n\n                                              Translated Audio Track\n           In this process, we have a Similarity and continuity engine that takes audio features from previous generated\n        segments, as well as features from the reference audio, and regenerates audio multiple times to ensure that the output\n        audio has similar emotions to the source segment, and when played with previous segments, it should sound\n        continuous.\n           Once this process is done for every segment, the generated audios are verified by our Human experts, and they\n        ensure the final Dubbing is perfect.\nlocalhost:3001                                                                                                                  6/6", "start_char_idx": 3254, "end_char_idx": 5475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-3": {"__data__": {"id_": "node-3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0f103fd-f885-4ad4-9216-8ecb77e40518", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44cce72dfc54eb3ebcb4be1566f418d29728824ef4849223f273db80f6c9bdca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5dec52b-4b2b-46fa-9dcd-fc9fa8e67296", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}}, "text": "(voice models)\n                                             Translation Segment#1                Orchestrator - Langchain- A custom\n                Translation #2-n                                              agent(AgentExecutor) - CATTM(Context Aware Text\n                                                                              Time Matcher) with 3 custom tools(@tools)-\n                            Target timeTranslation ContextTranslationVoice Model\n                                                                 Predicted time      Text Adjustment Tool - A tool built with\n           Same process for Segment #2-n  LLM Agent :: Text Adjustment::         LLMChain which takes input as contexual\n                               Low At          High At                           translation and gives out smaller or larger text as\n            Processed Translation Segment   Text Adjustment                      required.\n                                                             Updated Predicted time  GPT-4 evaluator Tool - A tool built with\n                                                  Validation Fail                load_evaluator which uses gpt-4 to critique the\n                                            Self Critiqe LLM                     quality of adjusted text given by Text Adjustment\n                                             Validation Pass                     tool.\n                                                    Voice Model                      Voice Model Tool - A tool that takes in text and\n                                                                                 speaker as input and spits out the time required to\n                                                                                 speak that text by that speaker.\n                                                                                  LLM - LLama2-7b for text adjustment(finetuned\n                                                                              using LORA), GPT-4 for evaluating outputs of llama2-\n                                                                              7b. HuggingFace - PEFT for finetuning llam2-7b using\n                                                                              LORA on custom dataset of shortening and lengthening\n                                                                              text.\n            Utilising those models, we run this process for every translated segment, and modify the translation based on the\n         acceptable limits of \u0394t, for every voice.\n            We have a LLM agent to adjust the translation, and a Self critique LLM to judge the modified translation on\n         parameters such as Grammar, Context loss, Colloquialism etc. Once the translations are modified, these are reviewed\n         by a Human in the loop for final validation.\n       Audio Generation\n            Once the translations are verified, we generate audio for each translated segment. To ensure that the final dubbing\n         is accurate, engaging and preserves the emotions from the original track, we have another LLM engine, that adjusts\n         translation based on continuous feedback, and regenerates audio multiple times to achieve this target.\nlocalhost:3001                                                                                                                         5/64/9/24, 7:56 PM               Verified Translation segments               React App\n           Translation segment #n\n                                           Reference segment\n             Audio generation                source lang::\n                    High 4\n          actual audio time       target time\n                 LLM                                                                   Technologies used -\n              Text Adjustment:\n                 Low $                        Audio features            ElevenLabs - For Text to Speech and voice cloning.\n             Modified Translation                                       Orchestrator - Langchain - CATTM agent\n                with Audio     Translation #n-1, #n-2\n            Audio Features      Audiofeatures                           Wav2vec and SpeechBrain - For Audio similarity and\n                                                                     continuity engine.\n                                   LLM : Similarity                     Python Libraries - Langchain, SpeechBrain, pydub,\n                                   and Continuity  Translation segment N| {n}moviepy\n                                    Engine::\n                           Failed    Passed\n                 Audio regenerationFinal Audio for segment #nSame process as Translated #n\n                                              Translated Audio Track\n           In this process, we have a Similarity and continuity engine that takes audio features from previous generated\n        segments, as well as features from the reference audio, and regenerates audio multiple times to ensure that the output\n        audio has similar emotions to the source segment, and when played with previous segments, it should sound\n        continuous.\n           Once this process is done for every segment, the generated audios are verified by our Human experts, and they\n        ensure the final Dubbing is perfect.\nlocalhost:3001                                                                                                                  6/6", "start_char_idx": 17145, "end_char_idx": 22620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ab5e9748-2e74-4c43-b1ab-5a3500df58eb": {"__data__": {"id_": "ab5e9748-2e74-4c43-b1ab-5a3500df58eb", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p1_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided describes a sophisticated platform for video and audio processing. The platform is hosted on an AWS EC2 Ubuntu 22 Server and utilizes a variety of technologies for its operations. The backend is developed using Python FastAPI, while the frontend is crafted with React. MongoDB is employed for storing video and user data, and S3 is used for hosting the audio and video files.\n\nThe platform integrates various large language models (LLMs) such as ChatGPT, Claude, and LLama2, and it logs all interactions with these models in Langfuse for better observability and performance monitoring. Additionally, it uses PostgreSQL for logging user interactions and NewRelic for monitoring the health of the instances.\n\nThe process begins with the uploading of a video, followed by the removal of the background using a dedicated tool deployed on the same server. This results in the separation of vocal and background audio tracks, ensuring a noise-free transcription process.\n\nTranscription is a key feature, where the audio file is converted into text, and speaker labels are assigned to each segment through speaker diarization. The platform uses a combination of speech-to-text APIs from Deepgram, AssemblyAI, and Gladia to achieve accurate transcriptions. An in-house ASR Engine further optimizes the output from these services to reduce errors and improve accuracy.\n\nThe ultimate goal of the platform is to produce lip-synchronized audio that matches the video content. To achieve this, the transcript segments are split into smaller sub-segments that align with", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77410c0d-927f-43b8-a8ca-63222e4ff4e3": {"__data__": {"id_": "77410c0d-927f-43b8-a8ca-63222e4ff4e3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p2_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The scene appears to be a snapshot of a user interface for a video editing or dubbing software application. The interface includes a video playback area, which is currently displaying what seems to be a travel or tourism-related segment. In the video, there are two individuals, a man and a woman, standing with a scenic coastal backdrop that suggests they might be in a location like the Amalfi Coast in Italy, as indicated by the transcription. The man is holding a microphone, which implies that he might be speaking or narrating, and both individuals are dressed casually, suitable for an outdoor setting.\n\nBelow the video playback, there is a timeline editor that shows the duration of the video segment, which is approximately 5.945 seconds long. The timeline includes colored segments that likely correspond to different parts of the video or audio that can be edited. The interface also displays transcription and translation text boxes, indicating that the software is used for translating video content. The transcription is in Italian, and there is a corresponding English translation provided.\n\nThe interface also includes various tools and options for editing, such as speaker mapping, audio dub, and segment adjustment. There are mentions of technologies and services used in the process, such as FFmpeg for audio extraction and breakpoints detection, and ASR (Automatic Speech Recognition) services like Assembly AI, Deepgram, and Gladia for transcription and audio processing. Additionally, Python libraries like moviepy and pydub are listed, which are commonly used for video and audio editing in", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a4afb82-969b-454e-be4a-1fbaac3c67dd": {"__data__": {"id_": "7a4afb82-969b-454e-be4a-1fbaac3c67dd", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p2_2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided suggests a workflow or process related to video editing and dubbing, particularly for a segment that features a scenic view of the Amalfi Coast in Italy. The process begins with a React application designed for emotion recognition, and it includes a video segment with a duration of approximately 5.945 seconds. The transcription indicates someone welcoming viewers to Italy and specifically to the beautiful Amalfi Coast.\n\nThe workflow involves several steps and technologies to enhance the video's audio and transcription quality. Audio is extracted from the video using FFmpeg, a powerful multimedia framework. This audio extraction allows for further manipulation, such as removing vocals from the track to isolate the background sounds or dialogue. Various automatic speech recognition (ASR) services like Assembly AI, Deepgram, and Gladia are used to generate an optimized transcript of the audio.\n\nOnce the transcription is complete and verified, it undergoes a translation process to ensure that the number of segments in the original transcript matches the number of segments in the translated version. This is crucial for accurate dubbing and synchronization of the video content.\n\nThe workflow diagram likely illustrates the connections between these different components and services, showing how they interact to produce a final dubbed video. The diagram would typically include nodes representing each service or technology, such as the ASR engines, vocal remover, and dubbing editor, with arrows indicating the flow of data and processing steps from the original video source to the final dubbed version.\n\nThe mention of Python libraries like moviepy and pyd", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3bbc1d7-4173-4f96-9527-f75571c3cdcc": {"__data__": {"id_": "e3bbc1d7-4173-4f96-9527-f75571c3cdcc", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p3_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content appears to be related to a Spanish-language video about luxury watchmaking, specifically discussing a caliber known as Powermatic 80. The speaker seems to be welcoming viewers and setting the stage for a discussion about the watch movement, its variants, and its significance in the watchmaking world. The aim of the video is to educate viewers on recognizing different watches with various movements, likely to enhance their understanding of luxury watches.\n\nAdditionally, there is a technical description of a process for translating and processing the video's transcript. This involves using an AI model for contextual translation, validating the translation, and grouping the transcript into manageable segments for processing. The translation is orchestrated using Langchain, with a few-shot example and an output parser to ensure the translation adheres to a specific format. A Semantic Mapping Engine is used to ensure consistency between the input and output segments. The translated segments are then assigned to an AI voice for auditory representation. The process is detailed and seems to be designed to handle complex translation tasks efficiently, ensuring that context is not lost in translation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1e8f185-c19e-4b22-a7e4-c4e20c9dba45": {"__data__": {"id_": "b1e8f185-c19e-4b22-a7e4-c4e20c9dba45", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p3_2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content provided appears to be a transcript of a video or a presentation, possibly related to watchmaking and the luxury watch industry. It includes a series of numbered statements in Spanish, each followed by an English translation. The statements seem to be part of a script or dialogue, discussing topics such as welcoming viewers, introducing the subject of the video, and delving into the specifics of watch movements, particularly the Powermatic 80 caliber, which is a well-known movement in the watch industry.\n\nThe text also outlines a process for translating and processing language data. It mentions the use of a Large Language Model (LLM) from OpenAI, specifically GPT-4, for contextual translation. The process involves splitting the input transcript into groups to manage the translation in segments, ensuring context is preserved. This is done with the help of technologies like Langchain, which orchestrates the translation process, and Python libraries for additional support.\n\nThe process also includes a validation step using an LLM Validator to ensure the input and output are balanced and a Semantic Mapping Engine to align the translated segments with the input transcript. The goal is to maintain the integrity of the translation across multiple segments, especially when dealing with more than 50 segments where performance issues can arise.\n\nThe final part of the process involves recombining the translated groups and assigning each segment a speaker, which is then mapped to an AI voice. This suggests that the translated content might be used for a multilingual video or audio presentation, where the AI voices", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "daf54686-2e6a-4f4e-90a5-1037e962dc9c": {"__data__": {"id_": "daf54686-2e6a-4f4e-90a5-1037e962dc9c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p4_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you're referring to presents a complex workflow for handling the challenges of translating text for audio generation across different languages. It outlines a system that accounts for the varying lengths of time it takes to speak the same text in different languages. This system includes a variety of technologies and processes to ensure that translations are not only accurate but also timed correctly to match the duration of the original audio.\n\nThe process begins with a postprocessing step that involves splitting translations and using machine learning models to predict the duration of audio for a given text. These models are developed for different voices and are trained to estimate how long it will take to speak a translated segment with about 80% accuracy.\n\nThe translation process is orchestrated by a custom agent that uses a Context Aware Text Time Matcher, which includes three custom tools to ensure the translated text matches the target time while maintaining the context. If the predicted time for a translated segment does not match the target, a Text Adjustment Tool is used to make the text shorter or longer as needed.\n\nThe quality of the adjusted text is then evaluated by a GPT-4 evaluator tool, which critiques the text based on grammar, context retention, colloquialism, and other parameters. A Voice Model Tool is also mentioned, which calculates the time required for a specific speaker to read the text.\n\nThe system also includes a LLM agent for text adjustment and a self-critique LLM to judge the modified translation. Human reviewers are involved in the final validation of the translations.\n\nFor audio", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1f42b2c-f518-4a53-9145-1c48c3cac530": {"__data__": {"id_": "f1f42b2c-f518-4a53-9145-1c48c3cac530", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p5_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided outlines a sophisticated process for generating and verifying translated audio tracks. It involves a series of steps to ensure that the audio produced matches the original in terms of timing, emotion, and continuity. The process uses advanced technologies such as text-to-speech, voice cloning, and audio similarity engines.\n\nThe workflow begins with a translation segment that is then processed to generate audio with the desired characteristics. The audio is adjusted for timing to match the source language's actual audio time. This modified translation with audio then goes through a similarity and continuity engine, which ensures that the new audio segment is emotionally similar to the reference and sounds continuous when played alongside previous segments.\n\nAfter the audio regeneration, the output is checked to determine if it has passed the necessary quality checks. If it fails, the audio is regenerated; if it passes, it becomes the final audio for that particular segment. This final audio is then verified by human experts to ensure the dubbing is flawless.\n\nThe entire process is supported by a variety of technologies and Python libraries, including ElevenLabs for text-to-speech, Langchain for orchestration, SpeechBrain and Wav2vec for audio processing, and other multimedia libraries like pydub and moviepy. The system seems to be accessible via a local web server, likely used for managing and executing the translation and audio generation tasks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"79de6ee3-106d-489b-968e-62eabfafaf87": {"doc_hash": "79fd1ab113dd48a3aa1a8c7a8067e7e8d02f14aa82947c24e4e268ff64ea23ab", "ref_doc_id": "node-0"}, "e102709b-3f80-4a93-97ef-5fefbea14a97": {"doc_hash": "44e52f936f05bcf549827d210c803e6aa294cafe1f7892461a6fd72c06e8d447", "ref_doc_id": "node-0"}, "fa2d269e-39ae-419f-befb-3d57028a9124": {"doc_hash": "90f6147dc2b78b7ebb35ed1fdf6818956438ca2362c0170da5bec093621098fa", "ref_doc_id": "node-0"}, "6a49b510-e611-4c14-97f7-35f191c2a2f5": {"doc_hash": "dca86935f0685b6baca85d815be0f70e19f2b47181c244a59c70a63d41252e2e", "ref_doc_id": "node-0"}, "0998e203-c32e-4720-85e3-f5ac6c6ef3d4": {"doc_hash": "663b34d5d062eb359f2f385a845a407dc0199314a9cf5c43b2bc3861f657d81e", "ref_doc_id": "node-0"}, "5a96d76e-692d-47c8-b8be-25bef3031512": {"doc_hash": "5d9643bf5af1b6545c42ee82c2cb29bae82c63b1972951767b1e814f5b2fa2d6", "ref_doc_id": "node-0"}, "3cec031e-c399-4ad9-8f17-77c16a24b62d": {"doc_hash": "aaab93af2de58f29f6b2154a147db026d7c23df308b157a9f550bcdf72165c65", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "ref_doc_id": "b0f103fd-f885-4ad4-9216-8ecb77e40518"}, "215f198e-2c13-4dfc-ad5f-55a9f09c806b": {"doc_hash": "0f7f75fc345315c3531611ac9c33d45e272d0e935db6cc97d684df189466720d", "ref_doc_id": "node-1"}, "a3932f1d-5174-4656-8b16-977e7dabe5d6": {"doc_hash": "72d97b448948bbf0aebbd6156c3e9351957bb43cdd70e84c29283399903f4225", "ref_doc_id": "node-1"}, "33046f41-e9af-4ff3-b1f7-79737fc163e8": {"doc_hash": "4590529e6de047fc5ec7ba5681510c1f1d6e075db4c935f5dcdefb7948144ee4", "ref_doc_id": "node-1"}, "4cc6d2ac-d1f3-4bb1-9029-8b9f5ee0560f": {"doc_hash": "ce730c9f2bf0a77bc48b426186abdefb4527953614dd89efa4925ea817111f3f", "ref_doc_id": "node-1"}, "1252e41b-96f5-416b-8d30-8db5fa7b0200": {"doc_hash": "affadbf1ad78eb820b199dda631fd447a8bd37c3e52a6b7031ea3dc39dad999e", "ref_doc_id": "node-1"}, "ddcbc84b-3c4c-4bc5-922d-46e788244b87": {"doc_hash": "8fe17b5cdd28b1237c9a2551f44a47726f3314201f1ce6a09e22c1fb25dfcb4c", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "ref_doc_id": "b0f103fd-f885-4ad4-9216-8ecb77e40518"}, "dcaecc73-0d2c-413b-9ff2-933f8941c60b": {"doc_hash": "f91ce012666760362d9a6ccb54efcc94996b1284fbb6f138d840f82bf2d2b243", "ref_doc_id": "node-2"}, "d1c3ca95-594c-4a15-9c16-7e1e614f645d": {"doc_hash": "c723334d49de28262266d58b21bbd65e57ab7304b8f4485b680da98d1bb2540a", "ref_doc_id": "node-2"}, "c8b1300e-16ec-4cfb-b28a-a1e49795ea20": {"doc_hash": "2111d9c7471ccf6983fb0245696eb8ab5fe89a65d1f27fdfaa3deb0ac97fe2c4", "ref_doc_id": "node-2"}, "39da8c3b-7e85-4d4a-aa8e-d735b0c94761": {"doc_hash": "1e9ab8847dd98e05862f5e5f8babb5d9e02ad955295762dc76214ec2949d4fcf", "ref_doc_id": "node-2"}, "5bfd716f-98c1-4cfd-b696-18f5dc9b0a2a": {"doc_hash": "6b22feb880cc20de54e8a3dcb920d5ec4f398a3c979a4ed1084988187f5573e2", "ref_doc_id": "node-2"}, "226c20cf-6cca-4951-a2b5-f718fb294e32": {"doc_hash": "6f5e95b841c18a850aef4aa12ee0be93d466d74aa60ed5a854f7d66dc7ab4465", "ref_doc_id": "node-2"}, "043032aa-2f21-4e1a-b09a-449a4a967c93": {"doc_hash": "05f631d4fcce0759b87695c8d09b9d6bd333ac4714fd4c13b1544a9d4531c4a9", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "ref_doc_id": "b0f103fd-f885-4ad4-9216-8ecb77e40518"}, "4f46f98e-9462-429e-97c2-acf33aae6fff": {"doc_hash": "e3d1361e1400459543958b5964eade7fb9be874015e7d9cb37265740cf98fa0f", "ref_doc_id": "node-3"}, "4e06aa50-e800-438f-9ca6-aeb533513341": {"doc_hash": "c30b03e96e1a992054d79d96345a534079cdbfd5a91be732dcc6ff9e3c64fd71", "ref_doc_id": "node-3"}, "0feae351-fc2c-4bf9-9066-0bfe55f3798f": {"doc_hash": "226b98d862e403ab81e12ae1e53ff8661c2bb3b5b7d36ee49325ffd51b6f16ac", "ref_doc_id": "node-3"}, "2d8df8ac-c04c-4d78-b46c-b2c4ec662b83": {"doc_hash": "0fa521b534bf1eac95beece63e68047be81e0ec681ef7ffb1b2a25cbfda15cfe", "ref_doc_id": "node-3"}, "247cdf5e-0afb-4c1c-b7d2-a408bedea1f5": {"doc_hash": "8c1fb58d8021ca969ef6ca644b6865211a5ec3da14d1ee784bc221eb862c2954", "ref_doc_id": "node-3"}, "84562e82-e49c-45f5-8c91-260e374807d7": {"doc_hash": "515c25b8aeb9d9ced21bb7ef7704aadeebb22967b46372f95f0753a9e2d0d246", "ref_doc_id": "node-3"}, "node-3": {"doc_hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "ref_doc_id": "b0f103fd-f885-4ad4-9216-8ecb77e40518"}, "ab5e9748-2e74-4c43-b1ab-5a3500df58eb": {"doc_hash": "d1ea41eb0b4e894de583db757215d43fb60fb9b3a08be23af7049fa49bdfb3f7"}, "77410c0d-927f-43b8-a8ca-63222e4ff4e3": {"doc_hash": "c133b5a35965f0ea182e8a647ee5e38492a80bc209cf155e7819c5a6148b7079"}, "7a4afb82-969b-454e-be4a-1fbaac3c67dd": {"doc_hash": "2d1633e788987dd9d935130841d6f105882053d16b2008ca64207cf4e0d78de1"}, "e3bbc1d7-4173-4f96-9527-f75571c3cdcc": {"doc_hash": "36d95d79731b6ff551b822d8616cbcdf01eba03d867d7dda28acabab85da2677"}, "b1e8f185-c19e-4b22-a7e4-c4e20c9dba45": {"doc_hash": "8a086b5483f22dbd5b638a9e0ecc57374de79634d8e66fac0ceadaf78e577632"}, "daf54686-2e6a-4f4e-90a5-1037e962dc9c": {"doc_hash": "1a2835623ebd8a5cb9a5c8d24f7669965f526e7f0f18971c1b25793b348954ec"}, "f1f42b2c-f518-4a53-9145-1c48c3cac530": {"doc_hash": "21a975d8016640557b97f6acf5ef07f6515aa6ed32a1e8e185b69660d22de661"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["79de6ee3-106d-489b-968e-62eabfafaf87", "e102709b-3f80-4a93-97ef-5fefbea14a97", "fa2d269e-39ae-419f-befb-3d57028a9124", "6a49b510-e611-4c14-97f7-35f191c2a2f5", "0998e203-c32e-4720-85e3-f5ac6c6ef3d4", "5a96d76e-692d-47c8-b8be-25bef3031512", "3cec031e-c399-4ad9-8f17-77c16a24b62d"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}, "b0f103fd-f885-4ad4-9216-8ecb77e40518": {"node_ids": ["node-0", "node-1", "node-2", "node-3"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}, "node-1": {"node_ids": ["215f198e-2c13-4dfc-ad5f-55a9f09c806b", "a3932f1d-5174-4656-8b16-977e7dabe5d6", "33046f41-e9af-4ff3-b1f7-79737fc163e8", "4cc6d2ac-d1f3-4bb1-9029-8b9f5ee0560f", "1252e41b-96f5-416b-8d30-8db5fa7b0200", "ddcbc84b-3c4c-4bc5-922d-46e788244b87"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}, "node-2": {"node_ids": ["dcaecc73-0d2c-413b-9ff2-933f8941c60b", "d1c3ca95-594c-4a15-9c16-7e1e614f645d", "c8b1300e-16ec-4cfb-b28a-a1e49795ea20", "39da8c3b-7e85-4d4a-aa8e-d735b0c94761", "5bfd716f-98c1-4cfd-b696-18f5dc9b0a2a", "226c20cf-6cca-4951-a2b5-f718fb294e32", "043032aa-2f21-4e1a-b09a-449a4a967c93"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}, "node-3": {"node_ids": ["4f46f98e-9462-429e-97c2-acf33aae6fff", "4e06aa50-e800-438f-9ca6-aeb533513341", "0feae351-fc2c-4bf9-9066-0bfe55f3798f", "2d8df8ac-c04c-4d78-b46c-b2c4ec662b83", "247cdf5e-0afb-4c1c-b7d2-a408bedea1f5", "84562e82-e49c-45f5-8c91-260e374807d7"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}}}